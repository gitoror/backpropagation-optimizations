{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation report - Tom Marini\n",
    "The goal of this report is to implement a basic backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation algorithm is an algorithm used to train a neural network. It actualizes the weights of the network during the training phase. The idea is to modify the weights in order to fit training data by analyzing the error made by the neural network.\n",
    "\n",
    "A neural network is made of neurons that have weights and a bias. Bias is often accounted as a weight, I decide to keep it distinct from weight in this report. Weights are linking neurons to each other. The neural network I want to implement is a network only made of fully connected layers. This means that the network is split into several layers, each containing several neurons. Since it is fully connected, each neuron from a layer is conntected to all the neurons of the previous and the next layers. \n",
    "\n",
    "Therefore, I start by implementing a Layer and a Neural Network class that fits this description. A Neural Network oject is made of several Layer and a Layer is a list of neurons that have weights and biases. Note that an activation function can also be provided.\n",
    "\n",
    "Then, a training phase consisit of forwarding an input through the network and backpropagate the prediction output error of the network until the first layer. A gradient descent is used to update weights. Translating this idea into code, the Neural Network class has a train function that makes the forwarding and backward phases of training data. The Layer class is doted of a forward function that computes the ouput for each layer and a backward that computes the error made by the each neuron. These steps can be repeated several epochs to improve the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer: # Fully connected layer (or dense layer)\n",
    "    \"\"\"\n",
    "    A fully connected layer in a neural network. \n",
    "    A layer has a number of neurons equal to the output_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, activation_function=lambda x: x, activation_derivative=lambda x: 1):\n",
    "        \"\"\" \n",
    "        input_size: the number of neurons in the previous layer\n",
    "        output_size: the number of neurons in the current layer\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.random.randn(output_size)\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        \n",
    "        self.neuron_values = None # the weighted sum of the inputs to the neuron (including the bias). Size is the same as the number of neurons in the layer.\n",
    "        self.output = None # the output of the neuron after applying the activation function. Size is the same as the number of neurons in the layer.\n",
    "        self.error = None # partial derivative of the cost function with respect to the neuron values. Size is the same as the nb of neurons in the layer.\n",
    "        self.bias_error = None # partial derivative of the cost function with respect to the biases. Size is the same as the nb of neurons in the layer.\n",
    "        self.weight_error = None # partial derivative of the cost function with respect to the weights. Size is the same as the weights matrix.\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\" \n",
    "        Computes the output of the layer given the input.\n",
    "        \"\"\"\n",
    "        self.neuron_values = np.dot(input, self.weights) + self.biases\n",
    "        self.output = self.activation_function(self.neuron_values)\n",
    "    \n",
    "    def backward(self, next_layer):\n",
    "        \"\"\"\n",
    "        Computes the error of the layer given the error of the next layer.\n",
    "        \"\"\"\n",
    "        self.error = np.dot(next_layer.error, next_layer.weights.T) * self.activation_derivative(self.neuron_values) \n",
    "\n",
    "    def print(self):\n",
    "        print(f'Weights: {self.weights}')\n",
    "        print(f'Biases: {self.biases}')\n",
    "        print(f'Activations: {self.neuron_values}')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network is a collection of layers. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, input_size, output_size, activation_function=lambda x: x, activation_derivative=lambda x: 1):\n",
    "        self.layers.append(Layer(input_size, output_size, activation_function, activation_derivative))\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        \"\"\" \n",
    "        Compute the output of the network given the input x.\n",
    "        \"\"\"\n",
    "        input = x\n",
    "        for layer in self.layers:\n",
    "            layer.forward(input)\n",
    "            input = layer.output\n",
    "    \n",
    "    def backpropagation(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the network given the input x and the target y.\n",
    "        x: input\n",
    "        y: target\n",
    "        \"\"\"\n",
    "        # Output layer. \n",
    "        # Compute the error of the output layer (assuming mean squared error). It depends on the cost function\n",
    "        self.layers[-1].error = y - self.layers[-1].output \n",
    "\n",
    "        # Hidden layers. Compute the errors of the hidden layers.\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            self.layers[i].backward(self.layers[i+1])\n",
    "\n",
    "        # Compute the cost variation with respect to the weights and biases\n",
    "        for i in range(len(self.layers)):\n",
    "            # Compute the weights errors\n",
    "            layer = self.layers[i]\n",
    "            if i == 0: # input layer\n",
    "                prev_layer_output = x\n",
    "            else:\n",
    "                prev_layer_output = self.layers[i-1].output\n",
    "            layer.weight_error = np.outer(prev_layer_output, layer.error)\n",
    "            # Compute the biases errors\n",
    "            layer.bias_error = layer.error\n",
    "\n",
    "        # Update the weights and biases\n",
    "        # This is the stochastic gradient descent because we update the weights and biases after each training example\n",
    "        for layer in self.layers:\n",
    "            layer.weights += learning_rate * layer.weight_error\n",
    "            layer.biases += learning_rate * layer.bias_error\n",
    "\n",
    "    def train(self, x_train, y_train, learning_rate, nb_epochs=1):\n",
    "        \"\"\" \n",
    "        Trains the network on the training data.\n",
    "        \"\"\"\n",
    "        for epoch in range(nb_epochs):\n",
    "            self.train_epoch(x_train, y_train, learning_rate)\n",
    "        \n",
    "    def train_epoch(self, x_train, y_train, learning_rate): \n",
    "        \"\"\"\n",
    "        Trains the network on the training data for one epoch.\n",
    "        \"\"\"\n",
    "        for x, y in zip(x_train, y_train): # x is an input, y is a target\n",
    "            self.forward_propagation(x)\n",
    "            self.backpropagation(x, y, learning_rate)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Applies the network to the input x.\n",
    "        \"\"\"    \n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[-1].output\n",
    "    \n",
    "    def print_network(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f'Layer {i}')\n",
    "            layer.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the backpropagation algorithm over some fictive data. I will use the XOR dataset to test the algorithm. The XOR dataset is a simple dataset that is not linearly separable. It is a good dataset to test the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0]\n",
      "Prediction: [1.]\n",
      "Target: [0]\n",
      "\n",
      "Input: [0 1]\n",
      "Prediction: [1.]\n",
      "Target: [1]\n",
      "\n",
      "Input: [1 0]\n",
      "Prediction: [1.]\n",
      "Target: [1]\n",
      "\n",
      "Input: [1 1]\n",
      "Prediction: [0.]\n",
      "Target: [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(2, 2, activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "nn.add_layer(2, 1, activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "\n",
    "nn.train(x_train, y_train, 0.1, 1000)\n",
    "\n",
    "for x, y in zip(x_train, y_train):\n",
    "    print(f'Input: {x}')\n",
    "    print(f'Prediction: {np.round(nn.predict(x))}')\n",
    "    print(f'Target: {y}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 epochs, the network is not able to predict the XOR dataset, some prediction are incorrect! Let's try to increase the number of epochs to see if the network can learn the XOR dataset.\n",
    "I tried to train the network on the XOR problem for different number of epochs. The network is able to predict the XOR dataset after 10000 epochs. We can notice that the loss function is decreasing over epochs. This is a good sign that the network is learning and it is confirmed by the prediction that are now correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training iterations: 1\n",
      "Input: [0 0], Prediction: [0.], Target: [0]\n",
      "Input: [0 1], Prediction: [0.], Target: [1]\n",
      "Input: [1 0], Prediction: [0.], Target: [1]\n",
      "Input: [1 1], Prediction: [0.], Target: [0]\n",
      "Loss: 0.28493118385080163\n",
      "\n",
      "Number of training iterations: 10\n",
      "Input: [0 0], Prediction: [0.], Target: [0]\n",
      "Input: [0 1], Prediction: [0.], Target: [1]\n",
      "Input: [1 0], Prediction: [0.], Target: [1]\n",
      "Input: [1 1], Prediction: [0.], Target: [0]\n",
      "Loss: 0.24883854837943403\n",
      "\n",
      "Number of training iterations: 100\n",
      "Input: [0 0], Prediction: [1.], Target: [0]\n",
      "Input: [0 1], Prediction: [0.], Target: [1]\n",
      "Input: [1 0], Prediction: [1.], Target: [1]\n",
      "Input: [1 1], Prediction: [0.], Target: [0]\n",
      "Loss: 0.24986954545577877\n",
      "\n",
      "Number of training iterations: 1000\n",
      "Input: [0 0], Prediction: [0.], Target: [0]\n",
      "Input: [0 1], Prediction: [1.], Target: [1]\n",
      "Input: [1 0], Prediction: [1.], Target: [1]\n",
      "Input: [1 1], Prediction: [0.], Target: [0]\n",
      "Loss: 0.024982939619820566\n",
      "\n",
      "Number of training iterations: 10000\n",
      "Input: [0 0], Prediction: [0.], Target: [0]\n",
      "Input: [0 1], Prediction: [1.], Target: [1]\n",
      "Input: [1 0], Prediction: [1.], Target: [1]\n",
      "Input: [1 1], Prediction: [0.], Target: [0]\n",
      "Loss: 6.5634352838248186e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    nn = NeuralNetwork()\n",
    "    nn.add_layer(2, 2, activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "    nn.add_layer(2, 1, activation_function=sigmoid, activation_derivative=sigmoid_derivative)\n",
    "    nn.train(x_train, y_train, 0.1, 10**i)\n",
    "    print(f'Number of training iterations: {10**i}')\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        print(f'Input: {x}, Prediction: {np.round(nn.predict(x))}, Target: {y}')\n",
    "\n",
    "    print(f'Loss: {nn.compute_loss(y_train, nn.predict(x_train))}')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
